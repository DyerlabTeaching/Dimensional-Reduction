---
title: "Ordination"
subtitle: ""
author: Rodney Dyer, PhD
format: revealjs
---

## Overall Impetus

There are many times when we have several columns of data recorded on indiviudal observations.

- Genotypes of individuals from seveal populations    
- Species counts across sampling locations    
- Climatic data (e.g., water/temperature) measured at several locations  


## Consequences

Some of the consequences of this is that we may have problems:  

- Visualizing more than 2-3 dimensions of the data  
- Understand which subset of the data are correlated (and thus redundant)  
- Trouble identifying signal from noise


:::{.fragment}
Are there methods for visualization and quantification of data like this?
:::


# Eigen Structure {background-color="black" background-image="media/contour.png" background-size="initial" background-position="right"}

## Eigen Desconstruction

> A method to factor high dimensional data into additive subcomponents

Just like you can factor the equation $-6x^2 + 5x + 4 = 0$ into the factors $(2x+1)(-3x+4)$, large data sets with $N$ rows and $K$ columns of data can be factored based upon their column-wise mean values, variances, and covariances between columns of data.



## Way Cool Matrix Algebra

Consider the matrix of data $X$ with $N$ rows and $K$ columns.  The variance of each of the $K$ data columns and their covariances, can be represented as an $KxK$ covariance matrix and is derived from this fancy formula.

&nbsp;

$S = X'[X'X]^{-1}X$

## &nbsp;

$$
S = \left[ \begin{array}{cccc}
\sigma_A^2 & \sigma_{AB}^2 & \ldots & \sigma_{AK}^2 \\
\sigma_{BA}^2 & \sigma_{B}^2 & \ldots & \sigma_{AK}^2 \\
\sigma_{CA}^2 & \sigma_{BC}^2 & \ddots & \sigma_{AK}^2 \\
\vdots & \vdots & \vdots & \vdots \\
\sigma_{KA}^2 & \sigma_{KD}^2 & \ldots & \sigma_{K}^2 \\
\end{array}\right]
$$


## Partitioning Variation & Covariation

So we can partition this matrix as:

$$
S = \sum_{i=1}^K \lambda_{i} \ell^\prime_i \ell_i
$$

Where:  

- $\lambda_i$ is a scaling number, and 

- $\ell_i$ is a 1xK vector of values.


## Principal Component Rotations

Consider the following data

```{r}
#| echo: false
library( tidyverse )
library( ggExtra )
data.frame( X = rnorm( 51, mean=12, sd = 3)) %>%
  mutate( Y = (3.2*X + rnorm(51, sd=4)) - 20) -> df

df %>%
  ggplot( aes(X,Y) ) + 
  geom_point() +
  theme_minimal(base_size=14) -> p 
p
```

## Marginal Distributions

```{r}
#| echo: false
ggMarginal( p, type="densigram")
```



## &nbsp; {background-color="white" background-image="https://live.staticflickr.com/65535/51206415998_3527374bc2_c_d.jpg" background-size="initial" background-position="center"}

## &nbsp; {background-color="white" background-image="https://live.staticflickr.com/65535/51206210046_4cc91ca0ea_c_d.jpg" background-size="initial" background-position="center"}

## &nbsp; {background-color="white" background-image="https://live.staticflickr.com/65535/51206975949_e08e018ffe_c_d.jpg" background-size="initial" background-position="center"}

## &nbsp; {background-color="white" background-image="https://live.staticflickr.com/65535/51206975979_22ceef0be6_c_d.jpg" background-size="initial" background-position="center"}

<!-- 
## &nbsp; {background-color="white" background-image="https://live.staticflickr.com/65535/51206994494_bcd4fb98c3_c_d.jpg" background-size="initial" background-position="center"}
--> 

## Creating Othoginal Data

The transformation you are doing is based upon applying a <font color="red">linear transformation</font> of the original data from its *previous* coordinate space into an *identically sized* new coordinate space.




## Performing a Principal Component Rotation

I've uploaded a copy of the `mv_genos.csv` file to Canvas that have the multivariate genotypes for 358 beetles from Baja California.

```{r, echo=TRUE, eval=FALSE}
data <- read_csv( "mv_genos.csv")
fit.pca <- princomp( data )
names( fit.pca )
```


```{r echo=FALSE}
library( gstudio )
data( arapat )
mv_genos <- to_mv( arapat ) 
rownames( mv_genos ) <- make.names(rownames(mv_genos), unique=TRUE)
colnames( mv_genos ) <- paste( "L", 1:ncol(mv_genos), sep="")
fit.pca <- princomp( mv_genos )
names( fit.pca )
```


## The Loadings

```{r}
fit.pca$loadings[,1]
```

```{r}

data.frame( Variable = colnames(mv_genos), 
            Loading = fit.pca$loadings[,1] ) |> 
  ggplot( aes(Variable, Loading) ) + 
  geom_col() + theme_minimal() + 
  scale_x_discrete( guide = guide_axis( n.dodge = 3 ) )
  
  
  # theme( axis.text.x = element_text(angle = 45 ) )

```

## Orthoginal

```{r}
data.frame( Variable = c( colnames(mv_genos), 
                          colnames(mv_genos)),
            Loading = c( fit.pca$loadings[,1],
                         fit.pca$loadings[,2] ),
            Axis = factor(rep( 1:2, each=ncol(mv_genos) ) ) ) |> 
  ggplot( aes(Variable, Loading) ) + 
  geom_col(position="dodge") + theme_minimal() + 
  scale_x_discrete( guide = guide_axis( n.dodge = 3 ) ) + 
  facet_grid(Axis ~ .)
```


## Angle Between Axes 


```{r}
a <- fit.pca$loadings[,1]
b <- fit.pca$loadings[,2]

angle <- function(x,y){
  dot.prod <- x%*%y 
  norm.x <- norm(x,type="2")
  norm.y <- norm(y,type="2")
  theta <- acos(dot.prod / (norm.x * norm.y))
  as.numeric(theta) * 180 / pi 
}

angle( a,b )



```




## The Contents 

```{r}
fit.pca
```



## Summary

```{r}
summary( fit.pca )
```

## Accumulation of Variation

```{r}
plot( fit.pca )
```

## Assymptotic Description

```{r}
x <- summary( fit.pca )$sdev^2
data.frame( Axis = 1:length(x), 
            Variance  = cumsum( (x/sum(x)) ) ) |> 
  ggplot( aes(Axis,Variance) ) + 
  geom_line() 
```




## Visualizing

```{r eval=FALSE}
predict( fit.pca ) %>% data.frame() %>% 
  mutate( Species = arapat$Species) -> pred.pca 
ggplot( pred.pca, aes(Comp.1,Comp.2,color=Species) ) + 
  geom_point()  +  theme( legend.position = "none")
```


```{r echo=FALSE}
predict( fit.pca ) %>%
  data.frame() %>%
  mutate( Species = arapat$Species) -> pred.pca 
ggplot( pred.pca, aes(Comp.1,Comp.2,color=Species) ) + 
  geom_point()  + 
  theme( legend.position = "none")
```



## Detailed Visualizations


```{r echo=FALSE}
library( factoextra )
fviz_pca_biplot( fit.pca, col.ind="#aaa" )
```



## Principal Coordinate Analysis

Like PCA but using distance matrices instead of raw data.

```{r}
D.Euc <- genetic_distance(arapat, mode="Euclidean")
dim(D.Euc)
fit.gendist <- prcomp( D.Euc, center = TRUE)
```


## &nbsp;
```{r}
summary( fit.gendist )
```



## &nbsp;

```{r echo=FALSE}
fviz_pca_biplot( fit.gendist )
```





# Hierarchical Clustering {background-color="black" background-image="media/contour.png" background-size="initial" background-position="right"}



## Clustering

:::: {.columns}

::: {.column width="50%"}
A technique to build a representation of similarity between objects.

- Supervised  

- Unsupervised

- Individual or Group Based
:::

::: {.column width="50%"}
![From www.nature.com/articles/s41467-020-20507-3](https://live.staticflickr.com/65535/51734955871_9c76562bd3_w_d.jpg)
:::

::::



## Greedy Hierarchical Clustering 

1. Find the two closest of the N objects.  
2. Merge them together and find the mean of their location, this is the new node.  
3. Find the two closes of the $N-1$ objects.  
4. Repeat until they are all done.  




## &nbsp;

![Help File for hclust](https://live.staticflickr.com/65535/51735192018_1444d8d533_o_d.png) 


## Visualizing From Distance Views

Requires that the `matrix` objects actually be turned into `dist` objects (which are `matrix` objects with constraints).

```{r}
dist( D.Euc[1:7,1:7] )
```


## Visualizing From Distance Views

:::: {.columns}

::: {.column width="50%"}
```{r}
d <- dist( D.Euc )
h <- hclust( d )
h
```
:::

::: {.column width="50%"}
```{r}
plot(h)
```
:::

::::






# Interactive Plots

```{r}
library( networkD3 )
dendroNetwork( h, height=400, 
               zoom=TRUE,
               textColour = c("red","green","orange","blue")[cutree(h,4)])
```











## Questions

If you have any questions, please feel free to either post them as an "Issue" on your copy of this GitHub Repository, post to the [Canvas](https://canvas.vcu.edu) discussion board for the class, or drop me an [email](mailto://rjdyer@vcu.edu).

![](media/peter_sellers.gif){.middle fig-alt="Peter Sellers looking bored" fig-align="center" width="500"}
:::
